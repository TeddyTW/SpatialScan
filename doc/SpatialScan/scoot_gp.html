<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.4" />
<title>SpatialScan.scoot_gp API documentation</title>
<meta name="description" content="Module to contain the GP Model Class" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>SpatialScan.scoot_gp</code></h1>
</header>
<section id="section-intro">
<p>Module to contain the GP Model Class</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Module to contain the GP Model Class&#34;&#34;&#34;

import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler

import pandas as pd
import numpy as np

import gpflow

# from gpflow.utilities import print_summary, set_trainable

import joblib


class GPLandscape:
    &#34;&#34;&#34;Class which manages the training, saving and loading of GP models for an array of detectors&#34;&#34;&#34;

    def __init__(self):
        self.models = None
        self.model_last_update_start = None
        self.model_last_update_end = None
        self.model_detector_id = None
        self.scalers = None

    def train_save_detector(
        self, scoot_df: pd.DataFrame, days_in_past: int, detector: str, kern=None
    ):
        &#34;&#34;&#34;Trains GP a to one detector, using a SCOOT dataframe format. The trained model is
        then saved in a directory along with other useful model data such as scalers
        Args:
            df: SCOOT dataframe of one detector used for training
            days_in_past: how many most recent days of past dataframe should be used for training
            detector: detector_id as string
            kern: Optional kernel choice
        Returns:
            last_update_start: date for which the detector was trained
            det: name of detector
        &#34;&#34;&#34;

        # set Y and X to our days_in_past used for fitting, reshape, and scale
        det = scoot_df[&#34;detector_id&#34;].unique()[0]
        Y = (
            scoot_df[&#34;n_vehicles_in_interval&#34;]
            .tail(n=24 * days_in_past)
            .to_numpy()
            .reshape(-1, 1)
        )
        last_update_start = scoot_df[&#34;measurement_end_utc&#34;].tail(n=24 * days_in_past).min()
        last_update_end = scoot_df[&#34;measurement_end_utc&#34;].tail(n=24 * days_in_past).max()
        Y = Y.astype(float)
        X = np.arange(1, len(Y) + 1, dtype=float).reshape(-1, 1)

        scaler = MinMaxScaler(feature_range=(-1, 1))
        y = scaler.fit_transform(Y)

        # iniialise kernel
        if kern is None:

            kern_pd = gpflow.kernels.Periodic(gpflow.kernels.SquaredExponential())
            kern_pw = gpflow.kernels.Periodic(gpflow.kernels.SquaredExponential())
            kern_se = gpflow.kernels.SquaredExponential()

            kern_pd.period.assign(24.0)
            kern_pw.period.assign(168.0)
            # kern_SE.lengthscales.assign(100)

            k = kern_pd * kern_pw + kern_se
        else:
            k = kern

        # fit our GP to X &amp; y
        model = gpflow.models.GPR(data=(X, y), kernel=k, mean_function=None)
        opt = gpflow.optimizers.Scipy()

        # optimise GP performance
        opt.minimize(
            model.training_loss, model.trainable_variables, options=dict(maxiter=100)
        )

        # save model as TF module under detector name
        frozen_model = gpflow.utilities.freeze(model)
        module_to_save = tf.Module()
        predict_fn = tf.function(
            frozen_model.predict_f,
            input_signature=[tf.TensorSpec(shape=[None, 1], dtype=tf.float64)],
        )
        module_to_save.predict = predict_fn

        # save path
        save_dir = str(&#34;gp_models/&#34; + detector + &#34;/&#34;)
        tf.saved_model.save(module_to_save, save_dir)

        scaler_filename = str(save_dir + &#34;scaler.gz&#34;)
        joblib.dump(scaler, scaler_filename)

        return last_update_start, last_update_end, det

    def train_save_landscape(self, scoot_df: pd.DataFrame, days_in_past: int):
        &#34;&#34;&#34;Trains GPs to multiple detectors passed to it in SCOOT dataframe format. Trained models are
        saved in directories

        Args:
            scoot_df: SCOOT dataframe used to train multiple detector models
            days_in_past: how manyt most recent days of past dataframe should be used for training
        &#34;&#34;&#34;

        detectors = scoot_df[&#34;detector_id&#34;].unique()

        last_update_starts = []
        last_update_ends = []
        saved_detectors = []

        for i, detector in enumerate(detectors, 1):
            single_detector_df = scoot_df[scoot_df[&#34;detector_id&#34;] == detector]

            try:
                date, end_date, det = self.train_save_detector(
                    single_detector_df, days_in_past, detector
                )
            except:
                print(detector, &#34; Matrix not invertible&#34;)
                continue

            last_update_starts.append(date)
            last_update_ends.append(end_date)
            saved_detectors.append(det)
            print(&#34;please wait: &#34;, i, &#34;/&#34;, len(detectors), end=&#34;\r&#34;)

        pd.DataFrame(
            {&#34;detectors&#34;: saved_detectors, &#34;last_update_start&#34;: last_update_starts, &#34;last_update_end&#34;: last_update_ends}
        ).to_csv(&#34;gp_models/det_date.csv&#34;, index=False)

    def load_landscape(self):
        &#34;&#34;&#34;Loads array of pre-trained models and model data as arays from file, and
        sets them to the class variable&#34;&#34;&#34;

        models = []
        scalers = []

        det_date = pd.read_csv(&#34;gp_models/det_date.csv&#34;, index_col=False)
        detectors = det_date[&#34;detectors&#34;].to_numpy()
        dates = det_date[&#34;last_update_start&#34;].astype(&#34;datetime64[h]&#34;).to_numpy()
        end_dates = det_date[&#34;last_update_start&#34;].astype(&#34;datetime64[h]&#34;).to_numpy()

        for i, detector in enumerate(detectors, 1):

            save_dir = str(&#34;gp_models/&#34; + detector + &#34;/&#34;)
            scaler_filename = str(save_dir + &#34;scaler.gz&#34;)
            models.append(tf.saved_model.load(save_dir))
            scalers.append(joblib.load(scaler_filename))
            print(&#34;please wait: &#34;, i, &#34;/&#34;, len(detectors), end=&#34;\r&#34;)

        self.models = models
        self.model_detector_id = detectors
        self.model_last_update_start = dates
        self.model_last_update_end = end_dates
        self.scalers = scalers

    def count_baseline(
        self, scoot_df: pd.DataFrame, detectors: list = None
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Produces a DataFrame where the count and baseline can be compared for use
        in scan statistics

    Args:
        scoot_df: Dataframe of processed SCOOT data which we want to compare to model
        detectors: List of detectors to compare to forecasts. Default behaviour
                   retrieves forecasts for all detectors present in input dataframe.

    Returns:
        forecast_df: Dataframe of SCOOT vehicle counts and baseline estimates&#34;&#34;&#34;

        pd.options.mode.chained_assignment = None

        if detectors is None:
            detectors = scoot_df[&#34;detector_id&#34;].drop_duplicates().to_numpy()

        framelist = []

        for i, detector in enumerate(detectors, 1):
            print(&#34;please wait: &#34;, i, &#34;/&#34;, len(detectors), end=&#34;\r&#34;)

            one_detector_df = scoot_df.loc[scoot_df[&#34;detector_id&#34;] == detector]

            start_of_trained_data = self.model_last_update_start[
                np.where(self.model_detector_id == detector)
            ]

            baseline_range = (
                (one_detector_df[&#34;measurement_end_utc&#34;] - start_of_trained_data[0])
                .to_numpy()
                .astype(&#34;timedelta64[h]&#34;)
            )
            baseline_range = baseline_range + np.timedelta64(1, &#34;h&#34;)

            loc = np.where(self.model_detector_id == detector)

            baseline_range = baseline_range.reshape(-1, 1)
            model = self.models[loc[0][0]]
            scaler = self.scalers[loc[0][0]]

            mean, var = model.predict(baseline_range)
            mean = scaler.inverse_transform(mean)
            var = scaler.inverse_transform(var)

            one_detector_df.rename(
                columns={&#34;n_vehicles_in_interval&#34;: &#34;count&#34;}, inplace=True,
            )

            one_detector_df = one_detector_df.assign(baseline=mean.flatten().tolist())
            one_detector_df = one_detector_df.assign(
                upper_99=(3 * np.sqrt(var.flatten()) + mean.flatten()).tolist()
            )
            one_detector_df = one_detector_df.assign(
                lower_99=(mean.flatten() - 3 * np.sqrt(var.flatten())).tolist()
            )
            one_detector_df = one_detector_df.assign(
                prediction_variance=var.flatten().tolist()
            )

            framelist.append(one_detector_df)

        return pd.concat(framelist)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="SpatialScan.scoot_gp.GPLandscape"><code class="flex name class">
<span>class <span class="ident">GPLandscape</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class which manages the training, saving and loading of GP models for an array of detectors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GPLandscape:
    &#34;&#34;&#34;Class which manages the training, saving and loading of GP models for an array of detectors&#34;&#34;&#34;

    def __init__(self):
        self.models = None
        self.model_last_update_start = None
        self.model_last_update_end = None
        self.model_detector_id = None
        self.scalers = None

    def train_save_detector(
        self, scoot_df: pd.DataFrame, days_in_past: int, detector: str, kern=None
    ):
        &#34;&#34;&#34;Trains GP a to one detector, using a SCOOT dataframe format. The trained model is
        then saved in a directory along with other useful model data such as scalers
        Args:
            df: SCOOT dataframe of one detector used for training
            days_in_past: how many most recent days of past dataframe should be used for training
            detector: detector_id as string
            kern: Optional kernel choice
        Returns:
            last_update_start: date for which the detector was trained
            det: name of detector
        &#34;&#34;&#34;

        # set Y and X to our days_in_past used for fitting, reshape, and scale
        det = scoot_df[&#34;detector_id&#34;].unique()[0]
        Y = (
            scoot_df[&#34;n_vehicles_in_interval&#34;]
            .tail(n=24 * days_in_past)
            .to_numpy()
            .reshape(-1, 1)
        )
        last_update_start = scoot_df[&#34;measurement_end_utc&#34;].tail(n=24 * days_in_past).min()
        last_update_end = scoot_df[&#34;measurement_end_utc&#34;].tail(n=24 * days_in_past).max()
        Y = Y.astype(float)
        X = np.arange(1, len(Y) + 1, dtype=float).reshape(-1, 1)

        scaler = MinMaxScaler(feature_range=(-1, 1))
        y = scaler.fit_transform(Y)

        # iniialise kernel
        if kern is None:

            kern_pd = gpflow.kernels.Periodic(gpflow.kernels.SquaredExponential())
            kern_pw = gpflow.kernels.Periodic(gpflow.kernels.SquaredExponential())
            kern_se = gpflow.kernels.SquaredExponential()

            kern_pd.period.assign(24.0)
            kern_pw.period.assign(168.0)
            # kern_SE.lengthscales.assign(100)

            k = kern_pd * kern_pw + kern_se
        else:
            k = kern

        # fit our GP to X &amp; y
        model = gpflow.models.GPR(data=(X, y), kernel=k, mean_function=None)
        opt = gpflow.optimizers.Scipy()

        # optimise GP performance
        opt.minimize(
            model.training_loss, model.trainable_variables, options=dict(maxiter=100)
        )

        # save model as TF module under detector name
        frozen_model = gpflow.utilities.freeze(model)
        module_to_save = tf.Module()
        predict_fn = tf.function(
            frozen_model.predict_f,
            input_signature=[tf.TensorSpec(shape=[None, 1], dtype=tf.float64)],
        )
        module_to_save.predict = predict_fn

        # save path
        save_dir = str(&#34;gp_models/&#34; + detector + &#34;/&#34;)
        tf.saved_model.save(module_to_save, save_dir)

        scaler_filename = str(save_dir + &#34;scaler.gz&#34;)
        joblib.dump(scaler, scaler_filename)

        return last_update_start, last_update_end, det

    def train_save_landscape(self, scoot_df: pd.DataFrame, days_in_past: int):
        &#34;&#34;&#34;Trains GPs to multiple detectors passed to it in SCOOT dataframe format. Trained models are
        saved in directories

        Args:
            scoot_df: SCOOT dataframe used to train multiple detector models
            days_in_past: how manyt most recent days of past dataframe should be used for training
        &#34;&#34;&#34;

        detectors = scoot_df[&#34;detector_id&#34;].unique()

        last_update_starts = []
        last_update_ends = []
        saved_detectors = []

        for i, detector in enumerate(detectors, 1):
            single_detector_df = scoot_df[scoot_df[&#34;detector_id&#34;] == detector]

            try:
                date, end_date, det = self.train_save_detector(
                    single_detector_df, days_in_past, detector
                )
            except:
                print(detector, &#34; Matrix not invertible&#34;)
                continue

            last_update_starts.append(date)
            last_update_ends.append(end_date)
            saved_detectors.append(det)
            print(&#34;please wait: &#34;, i, &#34;/&#34;, len(detectors), end=&#34;\r&#34;)

        pd.DataFrame(
            {&#34;detectors&#34;: saved_detectors, &#34;last_update_start&#34;: last_update_starts, &#34;last_update_end&#34;: last_update_ends}
        ).to_csv(&#34;gp_models/det_date.csv&#34;, index=False)

    def load_landscape(self):
        &#34;&#34;&#34;Loads array of pre-trained models and model data as arays from file, and
        sets them to the class variable&#34;&#34;&#34;

        models = []
        scalers = []

        det_date = pd.read_csv(&#34;gp_models/det_date.csv&#34;, index_col=False)
        detectors = det_date[&#34;detectors&#34;].to_numpy()
        dates = det_date[&#34;last_update_start&#34;].astype(&#34;datetime64[h]&#34;).to_numpy()
        end_dates = det_date[&#34;last_update_start&#34;].astype(&#34;datetime64[h]&#34;).to_numpy()

        for i, detector in enumerate(detectors, 1):

            save_dir = str(&#34;gp_models/&#34; + detector + &#34;/&#34;)
            scaler_filename = str(save_dir + &#34;scaler.gz&#34;)
            models.append(tf.saved_model.load(save_dir))
            scalers.append(joblib.load(scaler_filename))
            print(&#34;please wait: &#34;, i, &#34;/&#34;, len(detectors), end=&#34;\r&#34;)

        self.models = models
        self.model_detector_id = detectors
        self.model_last_update_start = dates
        self.model_last_update_end = end_dates
        self.scalers = scalers

    def count_baseline(
        self, scoot_df: pd.DataFrame, detectors: list = None
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Produces a DataFrame where the count and baseline can be compared for use
        in scan statistics

    Args:
        scoot_df: Dataframe of processed SCOOT data which we want to compare to model
        detectors: List of detectors to compare to forecasts. Default behaviour
                   retrieves forecasts for all detectors present in input dataframe.

    Returns:
        forecast_df: Dataframe of SCOOT vehicle counts and baseline estimates&#34;&#34;&#34;

        pd.options.mode.chained_assignment = None

        if detectors is None:
            detectors = scoot_df[&#34;detector_id&#34;].drop_duplicates().to_numpy()

        framelist = []

        for i, detector in enumerate(detectors, 1):
            print(&#34;please wait: &#34;, i, &#34;/&#34;, len(detectors), end=&#34;\r&#34;)

            one_detector_df = scoot_df.loc[scoot_df[&#34;detector_id&#34;] == detector]

            start_of_trained_data = self.model_last_update_start[
                np.where(self.model_detector_id == detector)
            ]

            baseline_range = (
                (one_detector_df[&#34;measurement_end_utc&#34;] - start_of_trained_data[0])
                .to_numpy()
                .astype(&#34;timedelta64[h]&#34;)
            )
            baseline_range = baseline_range + np.timedelta64(1, &#34;h&#34;)

            loc = np.where(self.model_detector_id == detector)

            baseline_range = baseline_range.reshape(-1, 1)
            model = self.models[loc[0][0]]
            scaler = self.scalers[loc[0][0]]

            mean, var = model.predict(baseline_range)
            mean = scaler.inverse_transform(mean)
            var = scaler.inverse_transform(var)

            one_detector_df.rename(
                columns={&#34;n_vehicles_in_interval&#34;: &#34;count&#34;}, inplace=True,
            )

            one_detector_df = one_detector_df.assign(baseline=mean.flatten().tolist())
            one_detector_df = one_detector_df.assign(
                upper_99=(3 * np.sqrt(var.flatten()) + mean.flatten()).tolist()
            )
            one_detector_df = one_detector_df.assign(
                lower_99=(mean.flatten() - 3 * np.sqrt(var.flatten())).tolist()
            )
            one_detector_df = one_detector_df.assign(
                prediction_variance=var.flatten().tolist()
            )

            framelist.append(one_detector_df)

        return pd.concat(framelist)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="SpatialScan.scoot_gp.GPLandscape.count_baseline"><code class="name flex">
<span>def <span class="ident">count_baseline</span></span>(<span>self, scoot_df: pandas.core.frame.DataFrame, detectors: list = None) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Produces a DataFrame where the count and baseline can be compared for use
in scan statistics</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scoot_df</code></strong></dt>
<dd>Dataframe of processed SCOOT data which we want to compare to model</dd>
<dt><strong><code>detectors</code></strong></dt>
<dd>List of detectors to compare to forecasts. Default behaviour
retrieves forecasts for all detectors present in input dataframe.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>forecast_df</code></dt>
<dd>Dataframe of SCOOT vehicle counts and baseline estimates</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def count_baseline(
    self, scoot_df: pd.DataFrame, detectors: list = None
) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Produces a DataFrame where the count and baseline can be compared for use
    in scan statistics

Args:
    scoot_df: Dataframe of processed SCOOT data which we want to compare to model
    detectors: List of detectors to compare to forecasts. Default behaviour
               retrieves forecasts for all detectors present in input dataframe.

Returns:
    forecast_df: Dataframe of SCOOT vehicle counts and baseline estimates&#34;&#34;&#34;

    pd.options.mode.chained_assignment = None

    if detectors is None:
        detectors = scoot_df[&#34;detector_id&#34;].drop_duplicates().to_numpy()

    framelist = []

    for i, detector in enumerate(detectors, 1):
        print(&#34;please wait: &#34;, i, &#34;/&#34;, len(detectors), end=&#34;\r&#34;)

        one_detector_df = scoot_df.loc[scoot_df[&#34;detector_id&#34;] == detector]

        start_of_trained_data = self.model_last_update_start[
            np.where(self.model_detector_id == detector)
        ]

        baseline_range = (
            (one_detector_df[&#34;measurement_end_utc&#34;] - start_of_trained_data[0])
            .to_numpy()
            .astype(&#34;timedelta64[h]&#34;)
        )
        baseline_range = baseline_range + np.timedelta64(1, &#34;h&#34;)

        loc = np.where(self.model_detector_id == detector)

        baseline_range = baseline_range.reshape(-1, 1)
        model = self.models[loc[0][0]]
        scaler = self.scalers[loc[0][0]]

        mean, var = model.predict(baseline_range)
        mean = scaler.inverse_transform(mean)
        var = scaler.inverse_transform(var)

        one_detector_df.rename(
            columns={&#34;n_vehicles_in_interval&#34;: &#34;count&#34;}, inplace=True,
        )

        one_detector_df = one_detector_df.assign(baseline=mean.flatten().tolist())
        one_detector_df = one_detector_df.assign(
            upper_99=(3 * np.sqrt(var.flatten()) + mean.flatten()).tolist()
        )
        one_detector_df = one_detector_df.assign(
            lower_99=(mean.flatten() - 3 * np.sqrt(var.flatten())).tolist()
        )
        one_detector_df = one_detector_df.assign(
            prediction_variance=var.flatten().tolist()
        )

        framelist.append(one_detector_df)

    return pd.concat(framelist)</code></pre>
</details>
</dd>
<dt id="SpatialScan.scoot_gp.GPLandscape.load_landscape"><code class="name flex">
<span>def <span class="ident">load_landscape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads array of pre-trained models and model data as arays from file, and
sets them to the class variable</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_landscape(self):
    &#34;&#34;&#34;Loads array of pre-trained models and model data as arays from file, and
    sets them to the class variable&#34;&#34;&#34;

    models = []
    scalers = []

    det_date = pd.read_csv(&#34;gp_models/det_date.csv&#34;, index_col=False)
    detectors = det_date[&#34;detectors&#34;].to_numpy()
    dates = det_date[&#34;last_update_start&#34;].astype(&#34;datetime64[h]&#34;).to_numpy()
    end_dates = det_date[&#34;last_update_start&#34;].astype(&#34;datetime64[h]&#34;).to_numpy()

    for i, detector in enumerate(detectors, 1):

        save_dir = str(&#34;gp_models/&#34; + detector + &#34;/&#34;)
        scaler_filename = str(save_dir + &#34;scaler.gz&#34;)
        models.append(tf.saved_model.load(save_dir))
        scalers.append(joblib.load(scaler_filename))
        print(&#34;please wait: &#34;, i, &#34;/&#34;, len(detectors), end=&#34;\r&#34;)

    self.models = models
    self.model_detector_id = detectors
    self.model_last_update_start = dates
    self.model_last_update_end = end_dates
    self.scalers = scalers</code></pre>
</details>
</dd>
<dt id="SpatialScan.scoot_gp.GPLandscape.train_save_detector"><code class="name flex">
<span>def <span class="ident">train_save_detector</span></span>(<span>self, scoot_df: pandas.core.frame.DataFrame, days_in_past: int, detector: str, kern=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains GP a to one detector, using a SCOOT dataframe format. The trained model is
then saved in a directory along with other useful model data such as scalers</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong></dt>
<dd>SCOOT dataframe of one detector used for training</dd>
<dt><strong><code>days_in_past</code></strong></dt>
<dd>how many most recent days of past dataframe should be used for training</dd>
<dt><strong><code>detector</code></strong></dt>
<dd>detector_id as string</dd>
<dt><strong><code>kern</code></strong></dt>
<dd>Optional kernel choice</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>last_update_start</code></dt>
<dd>date for which the detector was trained</dd>
<dt><code>det</code></dt>
<dd>name of detector</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_save_detector(
    self, scoot_df: pd.DataFrame, days_in_past: int, detector: str, kern=None
):
    &#34;&#34;&#34;Trains GP a to one detector, using a SCOOT dataframe format. The trained model is
    then saved in a directory along with other useful model data such as scalers
    Args:
        df: SCOOT dataframe of one detector used for training
        days_in_past: how many most recent days of past dataframe should be used for training
        detector: detector_id as string
        kern: Optional kernel choice
    Returns:
        last_update_start: date for which the detector was trained
        det: name of detector
    &#34;&#34;&#34;

    # set Y and X to our days_in_past used for fitting, reshape, and scale
    det = scoot_df[&#34;detector_id&#34;].unique()[0]
    Y = (
        scoot_df[&#34;n_vehicles_in_interval&#34;]
        .tail(n=24 * days_in_past)
        .to_numpy()
        .reshape(-1, 1)
    )
    last_update_start = scoot_df[&#34;measurement_end_utc&#34;].tail(n=24 * days_in_past).min()
    last_update_end = scoot_df[&#34;measurement_end_utc&#34;].tail(n=24 * days_in_past).max()
    Y = Y.astype(float)
    X = np.arange(1, len(Y) + 1, dtype=float).reshape(-1, 1)

    scaler = MinMaxScaler(feature_range=(-1, 1))
    y = scaler.fit_transform(Y)

    # iniialise kernel
    if kern is None:

        kern_pd = gpflow.kernels.Periodic(gpflow.kernels.SquaredExponential())
        kern_pw = gpflow.kernels.Periodic(gpflow.kernels.SquaredExponential())
        kern_se = gpflow.kernels.SquaredExponential()

        kern_pd.period.assign(24.0)
        kern_pw.period.assign(168.0)
        # kern_SE.lengthscales.assign(100)

        k = kern_pd * kern_pw + kern_se
    else:
        k = kern

    # fit our GP to X &amp; y
    model = gpflow.models.GPR(data=(X, y), kernel=k, mean_function=None)
    opt = gpflow.optimizers.Scipy()

    # optimise GP performance
    opt.minimize(
        model.training_loss, model.trainable_variables, options=dict(maxiter=100)
    )

    # save model as TF module under detector name
    frozen_model = gpflow.utilities.freeze(model)
    module_to_save = tf.Module()
    predict_fn = tf.function(
        frozen_model.predict_f,
        input_signature=[tf.TensorSpec(shape=[None, 1], dtype=tf.float64)],
    )
    module_to_save.predict = predict_fn

    # save path
    save_dir = str(&#34;gp_models/&#34; + detector + &#34;/&#34;)
    tf.saved_model.save(module_to_save, save_dir)

    scaler_filename = str(save_dir + &#34;scaler.gz&#34;)
    joblib.dump(scaler, scaler_filename)

    return last_update_start, last_update_end, det</code></pre>
</details>
</dd>
<dt id="SpatialScan.scoot_gp.GPLandscape.train_save_landscape"><code class="name flex">
<span>def <span class="ident">train_save_landscape</span></span>(<span>self, scoot_df: pandas.core.frame.DataFrame, days_in_past: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains GPs to multiple detectors passed to it in SCOOT dataframe format. Trained models are
saved in directories</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scoot_df</code></strong></dt>
<dd>SCOOT dataframe used to train multiple detector models</dd>
<dt><strong><code>days_in_past</code></strong></dt>
<dd>how manyt most recent days of past dataframe should be used for training</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_save_landscape(self, scoot_df: pd.DataFrame, days_in_past: int):
    &#34;&#34;&#34;Trains GPs to multiple detectors passed to it in SCOOT dataframe format. Trained models are
    saved in directories

    Args:
        scoot_df: SCOOT dataframe used to train multiple detector models
        days_in_past: how manyt most recent days of past dataframe should be used for training
    &#34;&#34;&#34;

    detectors = scoot_df[&#34;detector_id&#34;].unique()

    last_update_starts = []
    last_update_ends = []
    saved_detectors = []

    for i, detector in enumerate(detectors, 1):
        single_detector_df = scoot_df[scoot_df[&#34;detector_id&#34;] == detector]

        try:
            date, end_date, det = self.train_save_detector(
                single_detector_df, days_in_past, detector
            )
        except:
            print(detector, &#34; Matrix not invertible&#34;)
            continue

        last_update_starts.append(date)
        last_update_ends.append(end_date)
        saved_detectors.append(det)
        print(&#34;please wait: &#34;, i, &#34;/&#34;, len(detectors), end=&#34;\r&#34;)

    pd.DataFrame(
        {&#34;detectors&#34;: saved_detectors, &#34;last_update_start&#34;: last_update_starts, &#34;last_update_end&#34;: last_update_ends}
    ).to_csv(&#34;gp_models/det_date.csv&#34;, index=False)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="SpatialScan" href="index.html">SpatialScan</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="SpatialScan.scoot_gp.GPLandscape" href="#SpatialScan.scoot_gp.GPLandscape">GPLandscape</a></code></h4>
<ul class="">
<li><code><a title="SpatialScan.scoot_gp.GPLandscape.count_baseline" href="#SpatialScan.scoot_gp.GPLandscape.count_baseline">count_baseline</a></code></li>
<li><code><a title="SpatialScan.scoot_gp.GPLandscape.load_landscape" href="#SpatialScan.scoot_gp.GPLandscape.load_landscape">load_landscape</a></code></li>
<li><code><a title="SpatialScan.scoot_gp.GPLandscape.train_save_detector" href="#SpatialScan.scoot_gp.GPLandscape.train_save_detector">train_save_detector</a></code></li>
<li><code><a title="SpatialScan.scoot_gp.GPLandscape.train_save_landscape" href="#SpatialScan.scoot_gp.GPLandscape.train_save_landscape">train_save_landscape</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.4</a>.</p>
</footer>
</body>
</html>